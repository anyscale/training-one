{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06e549dc",
   "metadata": {},
   "source": [
    "# Ray Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dbc0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import and connect to ray:\n",
    "\n",
    "import ray\n",
    "ray.init()\n",
    "from ray import tune\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bc59d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking ray version\n",
    "ray.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1bf319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import PyTorch suite\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# import Seaborn statistical dataset package\n",
    "import seaborn as sns\n",
    "# data processing tools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "# metrics and \n",
    "from sklearn.metrics import max_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# execution timing, os utils\n",
    "import time\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68a7904",
   "metadata": {},
   "source": [
    "## We will be using the Flight dataset from Seaborn package for demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a5b729",
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_data = sns.load_dataset(\"flights\")\n",
    "flight_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2549dfec",
   "metadata": {},
   "source": [
    "## Data Exploration, normally a first step in any ML process\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33138e50",
   "metadata": {},
   "source": [
    "Some Data Processing to make it easier for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092d1269",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = flight_data['passengers'].values.astype(float)\n",
    "test_data_size = 12\n",
    "scaler = MinMaxScaler()\n",
    "all_data_normalized = scaler.fit_transform(all_data .reshape(-1, 1))\n",
    "train_data_normalized  = all_data_normalized[:-test_data_size]\n",
    "test_data_normalized = all_data_normalized[-test_data_size:]\n",
    "train_data_normalized = torch.FloatTensor(train_data_normalized).view(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dc8e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "fig_size[0] = 15\n",
    "fig_size[1] = 5\n",
    "plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "plt.title('Month vs Passenger')\n",
    "plt.ylabel('Total Passengers')\n",
    "plt.xlabel('Months')\n",
    "plt.grid(True)\n",
    "plt.autoscale(axis='x',tight=True)\n",
    "plt.plot(flight_data['passengers'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fa9544",
   "metadata": {},
   "source": [
    "## Some data preparations for LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d76f199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the passenger value is the input feature, we will be predicting the number of passengers. Need to encode the timeseries in to 12 months windows, normalize values etc.\n",
    "all_data = flight_data['passengers'].values.astype(float)\n",
    "test_data_size = 12\n",
    "scaler = MinMaxScaler()\n",
    "all_data_normalized = scaler.fit_transform(all_data .reshape(-1, 1))\n",
    "train_data_normalized  = all_data_normalized[:-test_data_size]\n",
    "test_data_normalized = all_data_normalized[-test_data_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cdbd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# timeseries sequence splicing. This can be optimized as a generator function based on input stream.\n",
    "train_data_normalized = torch.FloatTensor(train_data_normalized).view(-1)\n",
    "train_window = 12\n",
    "def create_inout_sequences(input_data, tw):\n",
    "    inout_seq = []\n",
    "    L = len(input_data)\n",
    "    for i in range(L-tw):\n",
    "        train_seq = input_data[i:i+tw]\n",
    "        train_label = input_data[i+tw:i+tw+1]\n",
    "        inout_seq.append((train_seq ,train_label))\n",
    "    return inout_seq\n",
    "train_inout_seq = create_inout_sequences(train_data_normalized, train_window)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a37ff5",
   "metadata": {},
   "source": [
    "## With data preparatin and data exploration done, we will start construct the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aef8577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the useal PyToch way -- class based model instantiation, define forward function.\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_layer_size=100, output_size=1):\n",
    "        super().__init__()\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_layer_size)\n",
    "\n",
    "        self.linear = nn.Linear(hidden_layer_size, output_size)\n",
    "\n",
    "        self.hidden_cell = (torch.zeros(1,1,self.hidden_layer_size),\n",
    "                            torch.zeros(1,1,self.hidden_layer_size))\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        lstm_out, self.hidden_cell = self.lstm(input_seq.view(len(input_seq) ,1, -1), self.hidden_cell)\n",
    "        predictions = self.linear(lstm_out.view(len(input_seq), -1))\n",
    "        return predictions[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c2ee65",
   "metadata": {},
   "source": [
    "### With a model defined, we go into defining the training behavior and testing behavior. In this tutorial, we define a simple trainable, leveraging python Duck Typing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93f8bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(model, optimizer, loss_function, epochs):\n",
    "    for i in range(epochs):\n",
    "        for seq, labels in train_inout_seq:\n",
    "            optimizer.zero_grad()\n",
    "            model.hidden_cell = (torch.zeros(1, 1, model.hidden_layer_size),\n",
    "                        torch.zeros(1, 1, model.hidden_layer_size))\n",
    "            y_pred = model(seq)\n",
    "            single_loss = loss_function(y_pred, labels)\n",
    "            single_loss.backward()\n",
    "            optimizer.step()\n",
    "        # optional -- you can add fine grain checkpoints here.\n",
    "        # below is only pseudo code.\n",
    "#             with tune.checkpoint_dir(step=epoch) as checkpoint_dir:\n",
    "#                 path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "#                 torch.save(\n",
    "#                     (model.state_dict(), optimizer.state_dict()), path)\n",
    "    print(f'epoch: {i:3} loss: {single_loss.item():10.10f}')\n",
    "def test(model, test_input, truth):\n",
    "    model.eval()\n",
    "    for i in range(train_window):\n",
    "        seq = torch.FloatTensor(test_input[-train_window:])\n",
    "        with torch.no_grad():\n",
    "            model.hidden = (torch.zeros(1, 1, model.hidden_layer_size),\n",
    "                        torch.zeros(1, 1, model.hidden_layer_size))\n",
    "            test_input.append(model(seq).item())\n",
    "    return max_error(test_input[train_window:],test_data_normalized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638d3a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Orchestrate behaves like a Trainable, taking in a config, then execute the train and test, remit evaluation metrics back to the main train function/actor.\n",
    "def orchestrate(config):  \n",
    "    epochs = 50\n",
    "    model = LSTM()\n",
    "    loss_function = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
    "    train(model, optimizer, loss_function, epochs)\n",
    "    test_inputs = train_data_normalized[-train_window:].tolist()\n",
    "    \n",
    "    error = test(model, test_inputs, test_data_normalized)\n",
    "    tune.report(error = error)\n",
    "    # This saves the model to the trial directory\n",
    "    torch.save(model.state_dict(), \"./model.pth\")\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72182eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the parameter search space, kick off the training.\n",
    "search_space = {\n",
    "    \"lr\":  tune.grid_search([0.1,0.01,0.001,0.0001]),\n",
    "}\n",
    "\n",
    "analysis = tune.run(orchestrate, config=search_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a204388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of how to run with trial scheduler. This example won't run, because the model is not reporting a mean accuracy\n",
    "# from ray.tune.schedulers import   ASHAScheduler\n",
    "# # AsyncHyperBandScheduler\n",
    "\n",
    "# analysis = tune.run(\n",
    "#     orchestrate,\n",
    "#     num_samples=20,\n",
    "#     scheduler=ASHAScheduler(metric=\"mean_accuracy\", mode=\"max\"),\n",
    "#     config=search_space)\n",
    "\n",
    "# Example of how to run the training with fine grain control on distribution and how much resource to allocate per trial.\n",
    "# tune.run(trainable, num_samples=100, resources_per_trial=tune.PlacementGroupFactory([{\"CPU\": 2, \"GPU\": 1}]))\n",
    "\n",
    "# We do have some Hyperparameter selection algoriths\n",
    "# from ray.tune.suggest.hyperopt import HyperOptSearch\n",
    "# tune.run(my_function, search_alg=HyperOptSearch(...))\n",
    "# https://docs.ray.io/en/latest/tune/api_docs/suggestion.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2496188",
   "metadata": {},
   "source": [
    "### Let's look at the diagram again"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27027d5",
   "metadata": {},
   "source": [
    "### Let's now look at the training result, and analyze the outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4211bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best config: \", analysis.get_best_config(\n",
    "    metric=\"error\", mode=\"min\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f4009c",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trial = analysis.get_best_trial(\"error\", \"min\", \"last\")\n",
    "print(best_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a2fa09",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = analysis.results_df\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a21954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Depends on whether your model takes in hyper parameters or not.\n",
    "# the trial specific hyper perameter can be retrieved via below\n",
    "# best_trial.config[\"l1\"], best_trial.config[\"l2\"]\n",
    "## Construct the model\n",
    "best_model = LSTM() \n",
    "\n",
    "logdir = best_trial.logdir\n",
    "state_dict = torch.load(os.path.join(logdir, \"model.pth\"))\n",
    "best_model.load_state_dict(state_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3ea5e2",
   "metadata": {},
   "source": [
    "### Alternatively, if you have enabled checkpointing, you can load more models from checkpoint dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c1293b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## construct checkpoint location, load from checkpoint\n",
    "# checkpoint_path = os.path.join(best_trial.checkpoint.value, \"checkpoint\")\n",
    "# model_state, optimizer_state = torch.load(checkpoint_path)\n",
    "# best_trained_model.load_state_dict(model_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9dd01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.eval()\n",
    "fut_pred = 12\n",
    "test_inputs = train_data_normalized[-train_window:].tolist()\n",
    "for i in range(fut_pred):\n",
    "    seq = torch.FloatTensor(test_inputs[-train_window:])\n",
    "    with torch.no_grad():\n",
    "        best_model.hidden = (torch.zeros(1, 1, best_model.hidden_layer_size),\n",
    "                        torch.zeros(1, 1, best_model.hidden_layer_size))\n",
    "        test_inputs.append(best_model(seq).item())\n",
    "\n",
    "actual_predictions = scaler.inverse_transform(np.array(test_inputs[train_window:] ).reshape(-1, 1))\n",
    "\n",
    "print(actual_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1133e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(132, 144, 1)\n",
    "\n",
    "plt.title('Month vs Passenger')\n",
    "plt.ylabel('Total Passengers')\n",
    "plt.grid(True)\n",
    "plt.autoscale(axis='x', tight=True)\n",
    "plt.plot(flight_data['passengers'])\n",
    "plt.plot(x,actual_predictions)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233c6b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Month vs Passenger')\n",
    "plt.ylabel('Total Passengers')\n",
    "plt.grid(True)\n",
    "plt.autoscale(axis='x', tight=True)\n",
    "\n",
    "plt.plot(flight_data['passengers'][-train_window:])\n",
    "plt.plot(x,actual_predictions)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b963f789",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
