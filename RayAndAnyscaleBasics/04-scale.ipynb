{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "defa3c40",
   "metadata": {},
   "source": [
    "# Scaling topics\n",
    "You put functions and Actors into Anyscale to scale them!\n",
    "Some things are automatic, and some merely easy if you know how."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036fb174",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import pandas as pd\n",
    "ray.init(address=\"auto\", namespace=\"scaling\", runtime_env={\"pip\":\"requirements.txt\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7343354",
   "metadata": {},
   "source": [
    "# Monte Carlo\n",
    "This simulation is useful because it gives you an opportunity to see how to scale \n",
    "loads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab9062d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def random_batch(batch_size):\n",
    "\n",
    "    total_in = 0\n",
    "    for i in range(batch_size):\n",
    "        x,y = random.uniform(-1,1), random.uniform(-1,1)\n",
    "        if (math.hypot(x,y) <= 1):\n",
    "            total_in += 1\n",
    "    return total_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed81b08",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "class PiApproximator():\n",
    "    def __init__(self):\n",
    "        self.approximations = []\n",
    "    def approximate(self, num_samples, batch_size):\n",
    "        start = time.time()\n",
    "        num_inside = []\n",
    "        for i in range(0, num_samples, batch_size):\n",
    "            num_inside.append(random_batch.remote(batch_size))\n",
    "        pi = (4 * sum(ray.get(num_inside)) / num_samples )\n",
    "        end = time.time()\n",
    "        self.approximations.append({ \"time\":end - start,\n",
    "            \"num_samples\":num_samples,\n",
    "            \"batch_size\":batch_size,\n",
    "            \"pi\":pi})\n",
    "        return pi\n",
    "    def get_approximations(self):\n",
    "        return pd.DataFrame(self.approximations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774dee19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ray.kill(ray.get_actor(\"approximator\"))\n",
    "approximator = PiApproximator.options(name=\"approximator\").remote()\n",
    "ray.get(approximator.approximate.remote(100, 1))\n",
    "ray.get(approximator.approximate.remote(1000, 1))\n",
    "ray.get(approximator.approximate.remote(1000, 10))\n",
    "ray.get(approximator.approximate.remote(1000, 100))\n",
    "ray.get(approximator.get_approximations.remote())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3dec176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use these numbers to start talking about efficiency\n",
    "#for x in range(5):\n",
    "    #ray.get([approximator.approximate.remote(i*100000,10000) for i in range(1,20)])\n",
    "df = ray.get(approximator.get_approximations.remote())\n",
    "df.plot(\"num_samples\",\"pi\", kind=\"scatter\")\n",
    "df.plot(\"num_samples\", \"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253093d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1cd21c5b",
   "metadata": {},
   "source": [
    "# Provisioning\n",
    "This example pre-provisions a cluster.  Autoscaling is a slow-reacting\n",
    "process.  If you need a lot of machines fast, this technique helped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235dc78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray.autoscaler.sdk\n",
    "INIT_CPUS=40\n",
    "tot_cpus = ray.cluster_resources()[\"CPU\"]\n",
    "if tot_cpus < INIT_CPUS:\n",
    "    ray.autoscaler.sdk.request_resources(num_cpus=INIT_CPUS)\n",
    "    # this kind of loop was required when CPU count was in the 1000s\n",
    "    while tot_cpus < INIT_CPUS:\n",
    "        print(f\"Total CPUs so far: {tot_cpus}\")\n",
    "        # wait some amount of time for machines to come up\n",
    "        time.sleep(15)\n",
    "        tot_cpus = ray.cluster_resources()[\"CPU\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b779f1be",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "## When you're ready, scale that cluster back down again.\n",
    "ray.autoscaler.sdk.request_resources(num_cpus=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e94e10",
   "metadata": {},
   "source": [
    "# Ray Serve\n",
    "Ray serve is a library for scaffolding highly parallel endpoints.\n",
    "Since it's integrated with the Ray framework, it's suitable for creating\n",
    "self-contained APIs for model inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4973631",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray import serve\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import requests\n",
    "from fastapi import FastAPI, Request\n",
    "BACKEND = \"resnet18:v0\"\n",
    "app = FastAPI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cf3d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "@serve.deployment(name=\"predictor\", route_prefix=\"/\")\n",
    "@serve.ingress(app)\n",
    "class ImageModel:\n",
    "    def __init__(self):\n",
    "        self.model = resnet18(pretrained=True).eval()\n",
    "        self.preprocessor = transforms.Compose([\n",
    "            transforms.Resize(224),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Lambda(lambda t: t[:3, ...]),  # remove alpha channel\n",
    "            transforms.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "    @app.post(\"/image_predict\")\n",
    "    async def predict(self, request : Request):\n",
    "        image_payload_bytes = await request.body()\n",
    "        pil_image = Image.open(BytesIO(image_payload_bytes))\n",
    "        print(\"[1/3] Parsed image data: {}\".format(pil_image))\n",
    "\n",
    "        pil_images = [pil_image]  # Our current batch size is one\n",
    "        input_tensor = torch.cat(\n",
    "            [self.preprocessor(i).unsqueeze(0) for i in pil_images])\n",
    "        print(\"[2/3] Images transformed, tensor shape {}\".format(\n",
    "            input_tensor.shape))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_tensor = self.model(input_tensor)\n",
    "        print(\"[3/3] Inference done!\")\n",
    "        return {\"class_index\": int(torch.argmax(output_tensor[0]))}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbb9d22",
   "metadata": {},
   "source": [
    "# Start Serve and use it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6807e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "serve.start(detached=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f254bd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up actor with 1 replicas\n",
    "ImageModel.options(num_replicas=1)\n",
    "# change the above line to '10' and see RPS go up\n",
    "ImageModel.deploy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a598658e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
